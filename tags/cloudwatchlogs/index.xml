<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>cloudwatchlogs on Go Buddy Go</title>
    <link>https://ecnedaced-seirots.github.io/tags/cloudwatchlogs/</link>
    <description>Recent content in cloudwatchlogs on Go Buddy Go</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja-jp</language>
    <lastBuildDate>Thu, 03 Feb 2022 21:00:00 +0900</lastBuildDate>
    <atom:link href="https://ecnedaced-seirots.github.io/tags/cloudwatchlogs/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Logs Insightsで秒毎のログイベント数をカウントするクエリ</title>
      <link>https://ecnedaced-seirots.github.io/post/b/cloudwatch-logs-insights-bin/</link>
      <pubDate>Thu, 03 Feb 2022 21:00:00 +0900</pubDate>
      <guid>https://ecnedaced-seirots.github.io/post/b/cloudwatch-logs-insights-bin/</guid>
      <description>&lt;p&gt;CloudWatch Logs Insightsで、秒毎のログイベント数をカウントしたい時。すっげぇ簡単なんだけどすぐ忘れるから。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Logs Insightsでロギングタイプを指定するクエリ</title>
      <link>https://ecnedaced-seirots.github.io/post/b/cloudwatch-logs-insights-tips/</link>
      <pubDate>Sat, 29 Jan 2022 23:30:00 +0900</pubDate>
      <guid>https://ecnedaced-seirots.github.io/post/b/cloudwatch-logs-insights-tips/</guid>
      <description>&lt;p&gt;CloudWatch Logs Insightsでログメッセージを抽出する時に一番よく使うのはキーワードでフィルタをかけるパターンだと思うが、ロギングタイプを指定することも可能と最近知った。&lt;/p&gt;</description>
    </item>
    <item>
      <title>AWS Lambdaのログ監視方法を考えてみる</title>
      <link>https://ecnedaced-seirots.github.io/post/b/aws-lambda-log-monitoring/</link>
      <pubDate>Sun, 23 Jan 2022 15:00:00 +0900</pubDate>
      <guid>https://ecnedaced-seirots.github.io/post/b/aws-lambda-log-monitoring/</guid>
      <description>&lt;p&gt;AWS Lambdaは関数が呼び出されると自動でCloudWatch Logsにログを吐く。このログの監視についてベストプラクティスを考えてみた。&lt;/p&gt;</description>
    </item>
    <item>
      <title>EKS FargateからFluent BitでCloudWatchにログ送信する</title>
      <link>https://ecnedaced-seirots.github.io/post/b/eks-fargate-fluent-bit/</link>
      <pubDate>Mon, 10 Jan 2022 20:00:00 +0900</pubDate>
      <guid>https://ecnedaced-seirots.github.io/post/b/eks-fargate-fluent-bit/</guid>
      <description>&lt;p&gt;過去記事でEKS FargateのPodを起動するところまでやってみた。今回はFargate PodからFluent Bit経由でCloudWatch Logsにログを送信してみる。&lt;/p&gt;</description>
    </item>
    <item>
      <title>CloudWatch LogsからS3にエクスポート(Lambda/Python)</title>
      <link>https://ecnedaced-seirots.github.io/post/a/cloudwatchlogs-export-lambda/</link>
      <pubDate>Sun, 19 Dec 2021 12:19:37 +0900</pubDate>
      <guid>https://ecnedaced-seirots.github.io/post/a/cloudwatchlogs-export-lambda/</guid>
      <description>CloudWatch Logsから、LmabdaでログをS3にエクスポートする。対象のロググループとバケット内の第一階層を引数で指定するようにした。今回の事例ではエクスポートの範囲は「前日0時〜実行当日の0時」となる。
参考
boto3 API Reference
LambdaよりCloudWatchログをS3に保存方法紹介
ちなみに過去マネコンから実行する手順書いた。
CloudWatchLogsからS3へログをエクスポートする 今回の検証に使用したアイテム（個人メモ） アイテム 名称 Lambda用IAMロール lambda_basic_execution Lambda関数 log-export-function S3バケット log-export-xxxxxxxx Lambda用IAMロールの権限はlogsフルアクセスのみ。S3もいると思ってたがなくてもできた。バケットポリシー側で許可しているからか。S3バケット名は環境変数で指定した。ちなみに対象バケットは、動作不可になるためオブジェクトロックを設定しないこと。
S3バケットポリシー（log-export-xxxxxxxx）
{ &amp;#34;Version&amp;#34;: &amp;#34;2012-10-17&amp;#34;, &amp;#34;Statement&amp;#34;: [ { &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34;, &amp;#34;Principal&amp;#34;: { &amp;#34;Service&amp;#34;: &amp;#34;logs.ap-northeast-1.amazonaws.com&amp;#34; }, &amp;#34;Action&amp;#34;: &amp;#34;s3:GetBucketAcl&amp;#34;, &amp;#34;Resource&amp;#34;: &amp;#34;arn:aws:s3:::log-export-xxxxxxxx&amp;#34; }, { &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34;, &amp;#34;Principal&amp;#34;: { &amp;#34;Service&amp;#34;: &amp;#34;logs.ap-northeast-1.amazonaws.com&amp;#34; }, &amp;#34;Action&amp;#34;: &amp;#34;s3:PutObject&amp;#34;, &amp;#34;Resource&amp;#34;: &amp;#34;arn:aws:s3:::log-export-xxxxxxxx/*&amp;#34;, &amp;#34;Condition&amp;#34;: { &amp;#34;StringEquals&amp;#34;: { &amp;#34;s3:x-amz-acl&amp;#34;: &amp;#34;bucket-owner-full-control&amp;#34; } } } ] } Lambdaコード(Python3.9)
import boto3 import collections from datetime import datetime, date, time, timedelta import os def lambda_handler(event, context): log_g = event.</description>
    </item>
    <item>
      <title>Terraform loop処理の応用編(4) - Lambda</title>
      <link>https://ecnedaced-seirots.github.io/post/a/terraform-loop-example-5/</link>
      <pubDate>Sun, 12 Dec 2021 22:00:00 +0900</pubDate>
      <guid>https://ecnedaced-seirots.github.io/post/a/terraform-loop-example-5/</guid>
      <description>今日のTerraform loopネタはLambda関数作成。ログ監視の一貫なので、CloudWatchLogsのロググループとサブスクリプションフィルタ作成も一緒にやる。
この例でのディレクトリ構成は以下の通り。lambda/upload配下のzipファイルはTerraformにより生成されたもので、初回は空である。
work_dir ├── config.tf #初期化ファイル ├── lambda │ ├── code │ │ ├── func001 │ │ │ └── lambda-func001.py │ │ ├── func002 │ │ │ └── lambda-func002.py │ │ └── func003 │ │ └── lambda-func003.py │ └── upload │ ├── lambda-func001.zip │ ├── lambda-func002.zip │ └── lambda-func003.zip ├── lambda.auto.tfvars ├── lambda_cwl.tf ├── terraform.tfvars #regionのみ定義 └── variables.tf 最初に、すべて定数で記述したパターン。
lambda_logs.tf（定数バージョン）
################################################# # Lambda archive data ################################################# data &amp;#34;archive_file&amp;#34; &amp;#34;data-lambda-func001&amp;#34; { type = &amp;#34;zip&amp;#34; source_dir = &amp;#34;lambda/code/func001&amp;#34; output_path = &amp;#34;lambda/upload/lambda-func001.</description>
    </item>
    <item>
      <title>CloudWatchLogsのログ監視 - サブスクリプションフィルタ &#43; Lambdaでメール送信(2)</title>
      <link>https://ecnedaced-seirots.github.io/post/a/cloudwatchlogs-send-filter-mail-2/</link>
      <pubDate>Sun, 14 Nov 2021 12:00:00 +0900</pubDate>
      <guid>https://ecnedaced-seirots.github.io/post/a/cloudwatchlogs-send-filter-mail-2/</guid>
      <description>表題の件、以下の過去記事に書いたが、この時点では送信される本文ががログメッセージだけとなっていて、通知メールとしては不十分なため本文もカスタマイズしてみた。
CloudWatchLogsのログ監視 - サブスクリプションフィルタ + Lambdaでメール送信
各種設定は冒頭の過去記事と同様のため割愛するとして、コードは変更前・後両方載せておく。
変更前：lambda_function.py(1)
import base64 import json import zlib import datetime import os import boto3 from botocore.exceptions import ClientError print(&amp;#39;Loading function&amp;#39;) def lambda_handler(event, context): data = zlib.decompress(base64.b64decode(event[&amp;#39;awslogs&amp;#39;][&amp;#39;data&amp;#39;]), 16+zlib.MAX_WBITS) data_json = json.loads(data) log_entire_json = json.loads(json.dumps(data_json[&amp;#34;logEvents&amp;#34;], ensure_ascii=False)) log_entire_len = len(log_entire_json) print(log_entire_json) for i in range(log_entire_len): log_json = json.loads(json.dumps(data_json[&amp;#34;logEvents&amp;#34;][i], ensure_ascii=False)) try: sns = boto3.client(&amp;#39;sns&amp;#39;) #SNS Publish publishResponse = sns.publish( TopicArn = os.environ[&amp;#39;SNS_TOPIC_ARN&amp;#39;], Message = log_json[&amp;#39;message&amp;#39;], Subject = os.environ[&amp;#39;ALARM_SUBJECT&amp;#39;] ) except Exception as e: print(e) 参考</description>
    </item>
    <item>
      <title>AWS監視の方式を整理したい</title>
      <link>https://ecnedaced-seirots.github.io/post/a/cloudwatch-monitoring-idea/</link>
      <pubDate>Sun, 07 Nov 2021 13:00:00 +0900</pubDate>
      <guid>https://ecnedaced-seirots.github.io/post/a/cloudwatch-monitoring-idea/</guid>
      <description>AWSで過去普通にやってた監視実装も、2,3年経つと（或いはそれより短い周期で）陳腐化する。以前は限られたサービスのリソース範囲でやれることをやっていればよかったが、今はSSM(Systems Manager)、Lambda、EventBridgeなどの「登場人物」が増えて、カスタマイズが可能になったからだ。やれることが増えた分、実装が複雑になる。その分チャレンジングな分野になって楽しめると言えないこともないが&amp;hellip;、時間が足りないんだ。頭痛ぇな、まったく。絡み合った糸をほぐすためにまとめてみる。
監視の種別としては大枠としてノード監視、閾値監視、ログ監視、プロセス監視、イベント監視と想定する。それぞれの実装方式が若干異なってくるため整理したい。
監視方式大枠 ノード監視
CloudWatchアラーム(ステータスチェック) ー＞ (EventBridgeルール) ー＞ Lambda ー＞ SNSトピック ー＞ メール送信 ※ハード障害等でインスタンスが落ちた時に発動される想定。手動で落とした時は発動しないので通知は来ない。
閾値監視
CloudWatchアラーム（閾値チェック） ー＞ (EventBridgeルール) ー＞ Lambda ー＞ SNSトピック ー＞ メール送信 ※EC2インスタンスのCPU使用率、ディスク使用率を想定。メモリ監視は別途カスタムメトリクスの実装がいる。
ログ監視
CloudWatchLogsでログ出力（サブスクリプションフィルタキーワード検知）ー＞ Lambda ー＞ SNSトピック ー＞ メール送信 ※これだけEventBridgeを使用しない。
プロセス監視
EC2インスタンス上のプロセス数監視に相当する。検索すると「プロセス落ちていたらインスタンス再起動」アクションの事例が多いが、今回やりたいのはメール通知だけ。一応メモっておくけど。 CWエージェント + SSM + インスタンス停止、Lamabdaなし
EC2上のプロセスを監視し自動復旧する
CWエージェント + SSM + 自動再起動、Lamabdaなし
AWSでプロセス監視を実装したい
CWエージェント + Lamabda + SSM + 自動再起動
EC2のプロセス監視と自動再起動
procstat事例
以下はSSMを使用せず、procstatプラグインを使用してプロセス監視する例。記事には監視設定以降の通知イベント事例はなし。
CloudWatch Agent でProcstatプラグインの利用が可能になりました
SSMを使わずCloudwatchでEC2上のプロセス監視をしてみる
以下は途中まで参照したところ（後半は有料サービスの案内）、アラームを作成するところまでわかりやすかった。であれば、閾値監視と同様にEBルールを挟んでLambdaをターゲットに指定 ー＞ SNSトピックに渡されてメール送信、でいけるはず。
CloudWatchでプロセス監視する手順をLinuxとWindowsともに詳しく紹介
イベント監視
イベント発生 ー＞ (EventBridgeルール) ー＞ Lambda ー＞ SNSトピック ー＞ メール送信 ※1.</description>
    </item>
    <item>
      <title>CloudWatchLogsのログ監視 - サブスクリプションフィルタ &#43; Lambdaでメール送信</title>
      <link>https://ecnedaced-seirots.github.io/post/a/cloudwatchlogs-send-filter-mail/</link>
      <pubDate>Sun, 31 Oct 2021 14:00:00 +0900</pubDate>
      <guid>https://ecnedaced-seirots.github.io/post/a/cloudwatchlogs-send-filter-mail/</guid>
      <description>AWSでのログ監視メール送信はサブスクリプションフィルタ + Lambda + SNSを使用するのがスタンダード。みんなやっていそうなことだが未経験だったのでやってみた。基本参考にしたのは王道クラメソさんの記事だったが、ちょっとわかりにくいところがあったので他の記事も合わせて参照して若干やり方変えつつ検証した。
今回はマネコン作業メインでやったが、CLIやTerraformなどAPI経由で実装する場合追加作業が発生するため注意が必要。(後述 補足事項に記載)
参考
CloudWatch Logs を文字列検知してログ内容をメールを送信してみた サブスクリプションフィルター版 【AWS】CloudWatch Logsからシステムログをメール通知する。 CloudWatch Logs のサブスクリプションフィルタを使って特定文字列を検知したログをEメール通知する ※CLI実装例 以下は今後の参考用
CloudWatchLogsの内容をフィルタリングしてLambdaで通知させたい ※除外キーワードをコードで記述する例 CloudWatchLogsからLambda経由でログメッセージを通知する ※Terraform実装例 処理概要 CWLにログが出力される CWLのサブスクリプションフィルタでキーワード検知 Lambda関数起動 SNSに連携される メール通知 作業概要 SNSトピック作成〜サブスクライブ Lambda用IAMロール作成 Lambda関数作成 ロググループ/ログストリーム作成 ロググループにサブスクリプションフィルタ作成 （配信先に3.のLambda関数を指定） テストログ送信〜メール通知確認 ※ログストリーム作成は検証時のみ。通常は自動生成される。
今回の検証に使用したアイテム（個人メモ） アイテム 名称 SNSトピック log-monitor-topic Lambda用IAMロール send-log-filter-role Lambda関数 send-log-filter-function サブスクリプションフィルタ send-log-filter やったこと SNSトピック作成〜サブスクライブ
過去記事:AWS EventBridge + SNSからのメール件名をカスタマイズするに書いたので省略。ここではCLIでやってるけどマネコンでも特にハマるところはない。アクセスポリシーはデフォルトにした。 Lambda用IAMロール作成
とりあえず以下のマネージドポリシーをアタッチ。 CloudEatchLogsFullAccess AmazonSNSFullAccess Lambda関数作成
(1) 参考ブログのコード貼り付け import base64 import json import zlib import datetime import os import boto3 from botocore.</description>
    </item>
    <item>
      <title>CloudWatchLogsからS3へログをエクスポートする</title>
      <link>https://ecnedaced-seirots.github.io/post/a/cloudwatchlogs-s3-export/</link>
      <pubDate>Sat, 23 Oct 2021 13:00:00 +0900</pubDate>
      <guid>https://ecnedaced-seirots.github.io/post/a/cloudwatchlogs-s3-export/</guid>
      <description>CloudWatchLogsからS3へログをエクスポートする。基本的に以下の通りにやればできるのだが、説明が冗長だったりわかりにくいところがあるので自分用に書いておく。IAMユーザ作成の手順とかいらん。親切のつもりだろうけど、無駄に記事が長くなって読む気が失せる&amp;hellip;
コンソールを使用してログデータを Amazon S3 にエクスポートする
概要。ログストリームのエクスポートはログストリームの画面ではなく、ロググループの画面から行う。事前にログエクスポート専用S3バケットを用意し、ドキュメントの通りにバケットポリシーを設定しておく。適当なランダム値のプレフィクスを作成し、バケットポリシーに反映する。バケットにオブジェクトロックがかかっていると動作しないので注意。
バケットポリシーサンプル
{ &amp;#34;Version&amp;#34;: &amp;#34;2012-10-17&amp;#34;, &amp;#34;Statement&amp;#34;: [ { &amp;#34;Action&amp;#34;: &amp;#34;s3:GetBucketAcl&amp;#34;, &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34;, &amp;#34;Resource&amp;#34;: &amp;#34;arn:aws:s3:::my-app-logs&amp;#34;, &amp;#34;Principal&amp;#34;: { &amp;#34;Service&amp;#34;: &amp;#34;logs.ap-northeast-1.amazonaws.com&amp;#34; } }, { &amp;#34;Action&amp;#34;: &amp;#34;s3:PutObject&amp;#34; , &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34;, &amp;#34;Resource&amp;#34;: &amp;#34;arn:aws:s3:::my-app-logs/sjh6dert3a/*&amp;#34;, &amp;#34;Condition&amp;#34;: { &amp;#34;StringEquals&amp;#34;: { &amp;#34;s3:x-amz-acl&amp;#34;: &amp;#34;bucket-owner-full-control&amp;#34; } }, &amp;#34;Principal&amp;#34;: { &amp;#34;Service&amp;#34;: &amp;#34;logs.ap-northeast-1.amazonaws.com&amp;#34; } } ] } 上記前提条件が整った上で、以下実行する。カッコ内は英語表記の場合。
対象のログストリーム画面上で、「アクション」(Actions)のプルダウンから、「データをAmazon S3に エクスポート」(Export data to Amazon S3)を選択。 次画面でバケット名、作成しておいたS3のプレフィクス名、ログストリーム、時間の範囲指定を行い、「エクスポート」実行 エクスポート先のS3では確かzip化された状態で格納されていたと思う。
ドキュメント中の表現一部「ランダムに生成されたプレフィクス」、これがわかりにくかった。ログエクスポート先のS3にランダム文字列のプレフィクスが存在するのが望ましいようだ。なぜ普通の文字列ではなくランダム値が望ましいのかはよくわからん。「生成されたランダム文字列」と書かれているもんだから、どこで生成してるんだ？と混乱した。これは自分で適当に決めた値でよい。
上記に書いた作業は必要に応じてアドホック的に行う対応であり、定常的対応であればLambdaなりshellなりでバッチ化するだろう。
追記
ということで、Lambdaによるログエクスポートの記事書いた。
CloudWatch LogsからS3にエクスポート(Lambda/Python) </description>
    </item>
  </channel>
</rss>
